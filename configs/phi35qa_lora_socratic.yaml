model:
  name: microsoft/Phi-3.5-mini-instruct
  kind: clm
  attn_implementation: sdpa

data:
  name: json
  data_file: data/custom_dataset/final_data.json
  split: null
  config_name: null
  train_split: train
  val_split: validation
  test_split: null
  max_train_samples: null
  max_val_samples: null #val is 0.2 of max train
  shuffle_train: true
  seed: 42
  preprocessor: SocraticPreprocessor

tokenizer:
  max_length: 2048

training:
  batch_size: 4
  lr: 5.0e-06
  lr_scheduler_type: cosine
  epochs: 10
  output_dir: output/1-2/socratic/phi2/
  eval_steps: 150
  gradient_accumulation_steps: 8
  gradient_checkpointing: true
  precision: bf16
  max_steps: 12000
  eval_strategy: steps



logging:
  report_to: tensorboard
  logging_dir: "./phi2/1-2/socratic/runs"
  logging_steps: 10
  save_steps: 250
  level: info
  save_strategy: steps

lora:
  rank: 16
  lora_alpha: 12
  lora_dropout: 0.05
  bias: none
  task_type: CAUSAL_LM

