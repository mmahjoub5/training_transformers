model:
  name: microsoft/phi-2
  kind: clm

data:
  name: dany0407/eli5_category
  split: null
  config_name: null
  train_split: train
  val_split: validation
  test_split: null
  max_train_samples: null
  max_val_samples: null
  shuffle_train: true
  seed: 42

tokenizer:
  max_length: 512

training:
  batch_size: 1
  lr: 3e-5
  epochs: 10
  output_dir: output/1229/phi2/
  weight_decay: 0.01
  eval_steps: 50
  gradient_accumulation_steps: 8
  gradient_checkpointing: true


logging:
  report_to: tensorboard
  logging_dir: "./phi2/1229/runs"
  log_every: 10
  save_every: 500
  level: info

lora:
  r: 16
  lora_alpha: 12
  lora_droput: 0.05
  bias: none
  task_type: CASUAL_LM
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
